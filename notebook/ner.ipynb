{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T16:39:33.519684Z",
     "start_time": "2025-06-28T16:39:33.502474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from underthesea import sent_tokenize, text_normalize\n",
    "from collections import Counter, defaultdict\n",
    "from common.writer import Writer\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "class NEREntityCounter:\n",
    "    def __init__(self, model_name=\"NlpHUST/ner-vietnamese-electra-base\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "        self.nlp = pipeline(\"ner\", model=self.model, tokenizer=self.tokenizer)\n",
    "\n",
    "    def aggregate_entities(self, ner_results):\n",
    "        entities = []\n",
    "        current_entity = \"\"\n",
    "        current_type = None\n",
    "\n",
    "        for token in ner_results:\n",
    "            word = token[\"word\"]\n",
    "            entity_type = token[\"entity\"]\n",
    "\n",
    "            if entity_type.startswith(\"B-\"):\n",
    "                if current_entity:\n",
    "                    entities.append({\"text\": current_entity.strip(), \"type\": current_type})\n",
    "                current_entity = word\n",
    "                current_type = entity_type[2:]\n",
    "\n",
    "            elif entity_type.startswith(\"I-\") and current_type == entity_type[2:]:\n",
    "                if word.startswith(\"##\"):\n",
    "                    current_entity += word[2:]\n",
    "                else:\n",
    "                    current_entity += \" \" + word\n",
    "            else:\n",
    "                if current_entity:\n",
    "                    entities.append({\"text\": current_entity.strip(), \"type\": current_type})\n",
    "                    current_entity = \"\"\n",
    "                    current_type = None\n",
    "\n",
    "        if current_entity:\n",
    "            entities.append({\"text\": current_entity.strip(), \"type\": current_type})\n",
    "\n",
    "        return entities\n",
    "\n",
    "    def count_entities_in_text(self, text):\n",
    "        text = text_normalize(text)\n",
    "        segs = sent_tokenize(text)\n",
    "        ner_results_batch = self.nlp(segs)\n",
    "\n",
    "        all_entities = []\n",
    "        for ner_results in ner_results_batch:\n",
    "            entities = self.aggregate_entities(ner_results)\n",
    "            all_entities.extend(entities)\n",
    "\n",
    "        counter = defaultdict(Counter)\n",
    "        for entity in all_entities:\n",
    "            normalized_text = entity[\"text\"].strip()\n",
    "            entity_type = entity[\"type\"]\n",
    "            counter[entity_type][normalized_text] += 1\n",
    "\n",
    "        return counter\n",
    "\n",
    "    def count_entities_in_dataframe(self, df, content_column=\"content\"):\n",
    "        total_counter = defaultdict(Counter)\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            text = row.get(content_column, \"\")\n",
    "            if pd.isna(text) or not text.strip():\n",
    "                continue\n",
    "            entity_counter = self.count_entities_in_text(text)\n",
    "            for entity_type, type_counter in entity_counter.items():\n",
    "                total_counter[entity_type].update(type_counter)\n",
    "\n",
    "        # Chuyển từ defaultdict(Counter) → DataFrame\n",
    "        rows = []\n",
    "        for entity_type, type_counter in total_counter.items():\n",
    "            for entity_text, count in type_counter.items():\n",
    "                rows.append({\n",
    "                    \"entity_text\": entity_text,\n",
    "                    \"entity_type\": entity_type,\n",
    "                    \"count\": count\n",
    "                })\n",
    "\n",
    "        df_entities = pd.DataFrame(rows)\n",
    "        return df_entities\n",
    "\n",
    "    def export_to_csv(self, entity_counter, output_path=\"entity_counts.csv\"):\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"entity_text\", \"entity_type\", \"count\"])\n",
    "            for entity_type, type_counter in entity_counter.items():\n",
    "                for entity_text, count in type_counter.items():\n",
    "                    writer.writerow([entity_type, entity_text, count])\n"
   ],
   "id": "fbc392c90014a022",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T16:39:33.537046Z",
     "start_time": "2025-06-28T16:39:33.528199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "writer = Writer(filepath=\"articles.csv\")\n",
    "df = writer.read_as_dataframe()\n",
    "print(df)"
   ],
   "id": "c933767999fbb863",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [crawled_time, published_time, title, content, author, url]\n",
      "Index: []\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T16:39:35.890765Z",
     "start_time": "2025-06-28T16:39:33.549906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ner_counter = NEREntityCounter()\n",
    "df_result = ner_counter.count_entities_in_dataframe(df.head(10))\n",
    "\n",
    "df_result_sorted = df_result.sort_values(by=\"count\", ascending=False)\n",
    "\n",
    "print(df_result_sorted)\n",
    "\n",
    "df_result_sorted.head(50).to_csv(\"top_50_entities.csv\", sep=\"|\", index=False, encoding=\"utf-8\")\n"
   ],
   "id": "dbc1dee80a5e19e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'count'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[32m/var/folders/w4/1410gfhx3f127pdfjtjvq2tc0000gn/T/ipykernel_9824/357702658.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m      1\u001B[39m ner_counter = NEREntityCounter()\n\u001B[32m      2\u001B[39m df_result = ner_counter.count_entities_in_dataframe(df.head(\u001B[32m10\u001B[39m))\n\u001B[32m      3\u001B[39m \n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m df_result_sorted = df_result.sort_values(by=\u001B[33m\"count\"\u001B[39m, ascending=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m      5\u001B[39m \n\u001B[32m      6\u001B[39m print(df_result_sorted)\n\u001B[32m      7\u001B[39m \n",
      "\u001B[32m~/Documents/Code/passinno/baomoi_scraper/.venv/lib/python3.11/site-packages/pandas/core/frame.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001B[39m\n\u001B[32m   7192\u001B[39m             )\n\u001B[32m   7193\u001B[39m         \u001B[38;5;28;01melif\u001B[39;00m len(by):\n\u001B[32m   7194\u001B[39m             \u001B[38;5;66;03m# len(by) == 1\u001B[39;00m\n\u001B[32m   7195\u001B[39m \n\u001B[32m-> \u001B[39m\u001B[32m7196\u001B[39m             k = self._get_label_or_level_values(by[\u001B[32m0\u001B[39m], axis=axis)\n\u001B[32m   7197\u001B[39m \n\u001B[32m   7198\u001B[39m             \u001B[38;5;66;03m# need to rewrap column in Series to apply key function\u001B[39;00m\n\u001B[32m   7199\u001B[39m             \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;28;01mis\u001B[39;00m \u001B[38;5;28;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[32m~/Documents/Code/passinno/baomoi_scraper/.venv/lib/python3.11/site-packages/pandas/core/generic.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(self, key, axis)\u001B[39m\n\u001B[32m   1907\u001B[39m             values = self.xs(key, axis=other_axes[\u001B[32m0\u001B[39m])._values\n\u001B[32m   1908\u001B[39m         \u001B[38;5;28;01melif\u001B[39;00m self._is_level_reference(key, axis=axis):\n\u001B[32m   1909\u001B[39m             values = self.axes[axis].get_level_values(key)._values\n\u001B[32m   1910\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1911\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m KeyError(key)\n\u001B[32m   1912\u001B[39m \n\u001B[32m   1913\u001B[39m         \u001B[38;5;66;03m# Check for duplicates\u001B[39;00m\n\u001B[32m   1914\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m values.ndim > \u001B[32m1\u001B[39m:\n",
      "\u001B[31mKeyError\u001B[39m: 'count'"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6f69a99a4a4cc100"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
